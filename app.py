import streamlit as st
import pandas as pd
import numpy as np
import google.generativeai as genai
import jieba
from collections import Counter
from itertools import combinations
import io
import json
import re

# ==========================================
# 0. å…¨åŸŸè¨­å®š
# ==========================================
if 'usage_count' not in st.session_state:
    st.session_state['usage_count'] = 0

# ==========================================
# 1. é é¢è¨­å®š
# ==========================================
st.set_page_config(
    page_title="Market Insight Miner v6.0",
    page_icon="ğŸ’",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .stDataFrame {font-size: 14px;}
    [data-testid="stSidebar"] {background-color: #f0f2f6;}
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. å´é‚Šæ¬„
# ==========================================
with st.sidebar:
    st.title("ğŸ’ Market Miner v6")
    st.caption("è©å½™çµæ§‹ Ã— å•†æ¥­åƒ¹å€¼åˆ†æ")
    st.markdown("---")
    
    api_key = st.text_input(
        "Gemini API Key", 
        type="password", 
        help="è«‹è¼¸å…¥ Google AI Studio æä¾›çš„ API Key"
    )
    st.caption("[å–å¾— API Key](https://aistudio.google.com/app/apikey)")
    
    if api_key:
        st.success("âœ… API Key å·²è¨­å®š")
        try:
            genai.configure(api_key=api_key)
        except Exception as e:
            st.error(f"API è¨­å®šå¤±æ•—: {e}")
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥ Key å•Ÿç”¨ AI")

    st.markdown("---")
    st.subheader("ğŸ§  æ¨¡å‹é¸æ“‡")
    selected_model = st.selectbox(
        "Gemini æ¨¡å‹",
        [
            "gemini-2.0-flash",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
        ],
        index=0,
        help="Flash è¼ƒå¿«è¼ƒä¾¿å®œï¼ŒPro è¼ƒç²¾æº–"
    )
    st.session_state['selected_model'] = selected_model
    
    st.markdown("---")
    st.metric("æœ¬æ¬¡ä½¿ç”¨æ¬¡æ•¸", st.session_state['usage_count'])
    
    st.markdown("---")
    mode = st.radio(
        "åŠŸèƒ½é¸æ“‡", 
        [
            "ğŸŒ± ç¨®å­é—œéµå­—ç”Ÿæˆ",
            "â›ï¸ è©å½™çµæ§‹åˆ†æ",
            "ğŸ”— è©å½™é—œè¯æ¢å‹˜"
        ]
    )
    st.markdown("---")
    st.caption("v6.0 | è©å½™æ·±åº¦åˆ†æç‰ˆ")

# ==========================================
# 3. æ ¸å¿ƒå‡½æ•¸
# ==========================================
def call_gemini(prompt, model_name=None):
    """å‘¼å« Gemini API"""
    if not api_key:
        return "âš ï¸ è«‹å…ˆè¼¸å…¥ API Key"
    
    # ä½¿ç”¨å‚³å…¥çš„æ¨¡å‹æˆ– sidebar é¸æ“‡çš„æ¨¡å‹
    use_model = model_name if model_name else st.session_state.get('selected_model', 'gemini-2.0-flash')
    
    try:
        model = genai.GenerativeModel(use_model)
        response = model.generate_content(prompt)
        st.session_state['usage_count'] += 1
        return response.text
    except Exception as e:
        return f"âŒ AI å‘¼å«å¤±æ•—: {str(e)}"


def clean_google_ads_data(df):
    """æ¸…ç† Google Ads CSV è³‡æ–™"""
    df.columns = df.columns.str.strip()
    
    # é—œéµå­—æ¬„ä½
    kw_col = next((c for c in df.columns if 'keyword' in c.lower() or 'é—œéµå­—' in c), None)
    if kw_col and kw_col != 'Keyword':
        df['Keyword'] = df[kw_col]
    
    # æœå°‹é‡
    search_col = next((c for c in df.columns if 'search' in c.lower() or 'æœå°‹' in c), None)
    if search_col:
        def clean_search(val):
            if pd.isna(val): return 0
            s = str(val).replace(',', '').replace('<', '').replace('>', '').strip()
            if '-' in s:
                try: 
                    parts = s.split('-')
                    return (float(parts[0]) + float(parts[1])) / 2
                except: return 0
            try: return float(s)
            except: return 0
        df['Avg. monthly searches'] = df[search_col].apply(clean_search)
    else: 
        df['Avg. monthly searches'] = 0

    # YoY æˆé•·ç‡
    yoy_col = next((c for c in df.columns if 'yoy' in c.lower() or 'change' in c.lower() or 'è®ŠåŒ–' in c), None)
    if yoy_col:
        def clean_yoy(val):
            if pd.isna(val): return 0
            s = str(val).replace('%', '').replace(',', '').replace('+', '').strip()
            if 'âˆ' in s: return 999
            if '--' in s or s == '': return 0
            try: return float(s)
            except: return 0
        df['YoY change'] = df[yoy_col].apply(clean_yoy)
    else: 
        df['YoY change'] = 0
    
    # High Bid (ç´…æµ·æŒ‡æ¨™)
    cpc_col = next((c for c in df.columns if ('high' in c.lower() and 'bid' in c.lower()) or 'é«˜ä½' in c), None)
    if cpc_col:
        def clean_bid(val):
            if pd.isna(val): return 0
            s = str(val).replace(',', '').replace('NT$', '').replace('$', '').strip()
            if '--' in s or s == '': return 0
            try: return float(s)
            except: return 0
        df['Top Page Bid (High)'] = df[cpc_col].apply(clean_bid)
    else: 
        df['Top Page Bid (High)'] = 0

    # Competition Index (è—æµ·æŒ‡æ¨™)
    comp_col = next((c for c in df.columns if 'index' in c.lower() and 'competition' in c.lower()), None)
    if not comp_col:
        comp_col = next((c for c in df.columns if 'ç«¶çˆ­' in c and 'ç´¢å¼•' in c), None)
    if comp_col:
        df['Competition Index'] = pd.to_numeric(df[comp_col], errors='coerce').fillna(50)
    else:
        df['Competition Index'] = 50
        
    return df


def tokenize_keywords(keywords_series, stop_words=None):
    """å°é—œéµå­—é€²è¡Œåˆ†è©"""
    if stop_words is None:
        stop_words = {
            'çš„', 'æ¨è–¦', 'èˆ‡', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'äº†', 'åŠ', ' ', 
            'ä»€ä¹ˆ', 'ä»€éº¼', 'æ€éº¼', 'å¦‚ä½•', 'å—', 'åƒ¹æ ¼', 'å¤šå°‘', 'éŒ¢',
            'ptt', 'dcard', 'å“ªè£¡', 'å¯ä»¥', 'è¦', 'æœƒ', 'èƒ½', 'å¥½'
        }
    
    all_tokens = []
    for kw in keywords_series.astype(str):
        tokens = list(jieba.cut(kw))
        filtered = [t for t in tokens if len(t) > 1 and t not in stop_words]
        all_tokens.extend(filtered)
    
    return all_tokens


def analyze_word_frequency(keywords_series, top_n=20):
    """è©é »åˆ†æ"""
    tokens = tokenize_keywords(keywords_series)
    freq = Counter(tokens).most_common(top_n)
    return pd.DataFrame(freq, columns=['è©å½™', 'é »æ¬¡'])


def analyze_cooccurrence(keywords_series, top_n=30):
    """
    å…±ç¾åˆ†æï¼šæ‰¾å‡ºç¶“å¸¸ä¸€èµ·å‡ºç¾çš„è©çµ„
    """
    stop_words = {
        'çš„', 'æ¨è–¦', 'èˆ‡', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'äº†', 'åŠ', ' ',
        'ä»€ä¹ˆ', 'ä»€éº¼', 'æ€éº¼', 'å¦‚ä½•', 'å—', 'åƒ¹æ ¼', 'å¤šå°‘', 'éŒ¢',
        'ptt', 'dcard', 'å“ªè£¡', 'å¯ä»¥', 'è¦', 'æœƒ', 'èƒ½', 'å¥½'
    }
    
    cooccur_counter = Counter()
    
    for kw in keywords_series.astype(str):
        tokens = list(jieba.cut(kw))
        filtered = [t for t in tokens if len(t) > 1 and t not in stop_words]
        # å–æ‰€æœ‰å…©å…©çµ„åˆ
        for pair in combinations(sorted(set(filtered)), 2):
            cooccur_counter[pair] += 1
    
    # è½‰æˆ DataFrame
    data = [(p[0], p[1], c) for p, c in cooccur_counter.most_common(top_n)]
    return pd.DataFrame(data, columns=['è©å½™A', 'è©å½™B', 'å…±ç¾æ¬¡æ•¸'])


def analyze_ngrams(keywords_series, n=2, top_k=20):
    """
    N-gram åˆ†æï¼šæ‰¾å‡ºå¸¸è¦‹çš„é€£çºŒè©çµ„
    """
    ngram_counter = Counter()
    
    for kw in keywords_series.astype(str):
        tokens = list(jieba.cut(kw))
        tokens = [t for t in tokens if len(t.strip()) > 0]
        
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i+n])
            # éæ¿¾å¤ªçŸ­æˆ–ç„¡æ„ç¾©çš„
            if all(len(t) > 1 for t in ngram):
                ngram_counter[ngram] += 1
    
    data = [(' '.join(ng), c) for ng, c in ngram_counter.most_common(top_k)]
    return pd.DataFrame(data, columns=[f'{n}-gram è©çµ„', 'å‡ºç¾æ¬¡æ•¸'])


def calculate_word_value(df, keywords_series):
    """
    è¨ˆç®—è©å½™å•†æ¥­åƒ¹å€¼
    çµåˆï¼šå‡ºç¾é »æ¬¡ Ã— å¹³å‡æœå°‹é‡ Ã— å¹³å‡å‡ºåƒ¹
    """
    stop_words = {
        'çš„', 'æ¨è–¦', 'èˆ‡', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'äº†', 'åŠ', ' ',
        'ä»€ä¹ˆ', 'ä»€éº¼', 'æ€éº¼', 'å¦‚ä½•', 'å—', 'åƒ¹æ ¼', 'å¤šå°‘', 'éŒ¢',
        'ptt', 'dcard', 'å“ªè£¡', 'å¯ä»¥', 'è¦', 'æœƒ', 'èƒ½', 'å¥½'
    }
    
    word_stats = {}
    
    for idx, kw in enumerate(keywords_series.astype(str)):
        tokens = list(jieba.cut(kw))
        filtered = [t for t in tokens if len(t) > 1 and t not in stop_words]
        
        row = df.iloc[idx] if idx < len(df) else None
        if row is None:
            continue
            
        search_vol = row.get('Avg. monthly searches', 0)
        bid = row.get('Top Page Bid (High)', 0)
        yoy = row.get('YoY change', 0)
        
        for token in filtered:
            if token not in word_stats:
                word_stats[token] = {
                    'count': 0,
                    'total_search': 0,
                    'total_bid': 0,
                    'total_yoy': 0
                }
            word_stats[token]['count'] += 1
            word_stats[token]['total_search'] += search_vol
            word_stats[token]['total_bid'] += bid
            word_stats[token]['total_yoy'] += yoy
    
    # è¨ˆç®—æŒ‡æ¨™
    results = []
    for word, stats in word_stats.items():
        count = stats['count']
        avg_search = stats['total_search'] / count if count > 0 else 0
        avg_bid = stats['total_bid'] / count if count > 0 else 0
        avg_yoy = stats['total_yoy'] / count if count > 0 else 0
        
        # å•†æ¥­åƒ¹å€¼åˆ†æ•¸ = é »æ¬¡æ¬Šé‡ Ã— æœå°‹é‡æ¬Šé‡ Ã— å‡ºåƒ¹æ¬Šé‡
        value_score = (
            np.log1p(count) * 0.3 +
            np.log1p(avg_search) * 0.4 +
            np.log1p(avg_bid) * 0.3
        ) * 10
        
        results.append({
            'è©å½™': word,
            'å‡ºç¾æ¬¡æ•¸': count,
            'å¹³å‡æœå°‹é‡': round(avg_search, 0),
            'å¹³å‡å‡ºåƒ¹': round(avg_bid, 1),
            'å¹³å‡YoY': round(avg_yoy, 1),
            'å•†æ¥­åƒ¹å€¼åˆ†': round(value_score, 2)
        })
    
    result_df = pd.DataFrame(results)
    return result_df.sort_values('å•†æ¥­åƒ¹å€¼åˆ†', ascending=False)


def analyze_word_trends(df, keywords_series):
    """
    è©å½™è¶¨å‹¢åˆ†ç¾¤ï¼šä¸Šå‡è©ã€ä¸‹é™è©ã€ç©©å®šè©
    """
    stop_words = {
        'çš„', 'æ¨è–¦', 'èˆ‡', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'äº†', 'åŠ', ' ',
        'ä»€ä¹ˆ', 'ä»€éº¼', 'æ€éº¼', 'å¦‚ä½•', 'å—', 'åƒ¹æ ¼', 'å¤šå°‘', 'éŒ¢',
        'ptt', 'dcard', 'å“ªè£¡', 'å¯ä»¥', 'è¦', 'æœƒ', 'èƒ½', 'å¥½'
    }
    
    word_yoy = {}
    
    for idx, kw in enumerate(keywords_series.astype(str)):
        tokens = list(jieba.cut(kw))
        filtered = [t for t in tokens if len(t) > 1 and t not in stop_words]
        
        row = df.iloc[idx] if idx < len(df) else None
        if row is None:
            continue
            
        yoy = row.get('YoY change', 0)
        search_vol = row.get('Avg. monthly searches', 0)
        
        for token in filtered:
            if token not in word_yoy:
                word_yoy[token] = {'yoy_values': [], 'search_values': []}
            word_yoy[token]['yoy_values'].append(yoy)
            word_yoy[token]['search_values'].append(search_vol)
    
    results = []
    for word, data in word_yoy.items():
        avg_yoy = np.mean(data['yoy_values'])
        avg_search = np.mean(data['search_values'])
        count = len(data['yoy_values'])
        
        # åˆ†ç¾¤
        if avg_yoy > 20:
            trend = 'ğŸš€ ä¸Šå‡'
        elif avg_yoy < -20:
            trend = 'ğŸ“‰ ä¸‹é™'
        else:
            trend = 'â¡ï¸ ç©©å®š'
        
        results.append({
            'è©å½™': word,
            'å¹³å‡YoY': round(avg_yoy, 1),
            'å¹³å‡æœå°‹é‡': round(avg_search, 0),
            'å‡ºç¾æ¬¡æ•¸': count,
            'è¶¨å‹¢': trend
        })
    
    return pd.DataFrame(results).sort_values('å¹³å‡YoY', ascending=False)


def parse_intent_data(uploaded_json):
    """è§£æ SERP é›·é”çš„æ„åœ–ç ”ç©¶çµæœ"""
    try:
        if isinstance(uploaded_json, str):
            data = json.loads(uploaded_json)
        else:
            data = json.load(uploaded_json)
        
        # æ”¯æ´é™£åˆ—æˆ–å–®ä¸€ç‰©ä»¶
        if isinstance(data, list):
            return data
        return [data]
    except Exception as e:
        return None


# ==========================================
# 4. æ¨¡å¼ä¸€ï¼šç¨®å­é—œéµå­—ç”Ÿæˆ
# ==========================================
if mode == "ğŸŒ± ç¨®å­é—œéµå­—ç”Ÿæˆ":
    st.header("ğŸŒ± Google Ads ç¨®å­é—œéµå­—ç”Ÿæˆ")
    st.info("è¼¸å…¥ä¸»é¡Œï¼ŒAI ç”Ÿæˆ 3 çµ„ç­–ç•¥é—œéµå­—")
    
    topic = st.text_input("ç”¢å“æˆ–ä¸»é¡Œ", placeholder="ä¾‹å¦‚ï¼šç›Šç”ŸèŒã€ç©ºæ°£æ¸…æ·¨æ©Ÿ")
    
    if topic and st.button("ğŸš€ ç”Ÿæˆç­–ç•¥", type="primary"):
        if not api_key:
            st.error("è«‹å…ˆè¼¸å…¥ API Key")
        else:
            with st.spinner("AI è¦åŠƒä¸­..."):
                prompt = f"""
                ä¸»é¡Œï¼šã€Œ{topic}ã€
                
                è«‹ç”Ÿæˆ 3 çµ„ Google Keyword Planner ç¨®å­é—œéµå­—ï¼ˆæ¯çµ„ 10 å€‹ï¼‰ã€‚
                
                æ ¼å¼è¦æ±‚ï¼ˆMarkdownï¼‰ï¼š
                
                ### 1. ã€å¸‚å ´å¤§ç›¤çµ„ã€‘æµé‡å‹
                (10å€‹å“é¡å¤§è©ï¼Œé€—è™Ÿåˆ†éš”)
                
                ### 2. ã€ç²¾æº–è½‰åŒ–çµ„ã€‘ç—›é»å‹
                (10å€‹åŠŸæ•ˆ/å•é¡Œ/å•å¥è©ï¼Œé€—è™Ÿåˆ†éš”)
                
                ### 3. ã€ç«¶å“æ””æˆªçµ„ã€‘è—æµ·å‹
                (10å€‹ç«¶å“æˆ–æ›¿ä»£æ–¹æ¡ˆè©ï¼Œé€—è™Ÿåˆ†éš”)
                
                ç›´æ¥è¼¸å‡ºï¼Œä¸è¦å¤šé¤˜èªªæ˜ã€‚
                """
                result = call_gemini(prompt)
                st.markdown(result)


# ==========================================
# 5. æ¨¡å¼äºŒï¼šè©å½™çµæ§‹åˆ†æ
# ==========================================
elif mode == "â›ï¸ è©å½™çµæ§‹åˆ†æ":
    st.header("â›ï¸ è©å½™çµæ§‹ Ã— å•†æ¥­åƒ¹å€¼åˆ†æ")
    
    # ä¸Šå‚³å€åŸŸ
    col_upload1, col_upload2 = st.columns(2)
    
    with col_upload1:
        st.subheader("ğŸ“Š Google Ads CSVï¼ˆæ”¯æ´å¤šæª”ï¼‰")
        uploaded_csvs = st.file_uploader(
            "ä¸Šå‚³ Keyword Planner CSV",
            type=['csv'],
            key="csv_upload",
            accept_multiple_files=True
        )
    
    with col_upload2:
        st.subheader("ğŸ¯ æ„åœ–ç ”ç©¶çµæœï¼ˆé¸å¡«ï¼‰")
        intent_input_method = st.radio(
            "è¼¸å…¥æ–¹å¼",
            ["ä¸Šå‚³ JSON", "è²¼ä¸Šæ–‡å­—"],
            horizontal=True
        )
        
        intent_data = None
        if intent_input_method == "ä¸Šå‚³ JSON":
            uploaded_intent = st.file_uploader(
                "ä¸Šå‚³ SERP é›·é” JSON",
                type=['json'],
                key="intent_upload"
            )
            if uploaded_intent:
                intent_data = parse_intent_data(uploaded_intent)
                if intent_data:
                    st.success(f"âœ… å·²è¼‰å…¥ {len(intent_data)} ç­†æ„åœ–è³‡æ–™")
        else:
            intent_text = st.text_area(
                "è²¼ä¸Š SERP é›·é” JSON",
                height=150,
                placeholder='[{"Keyword": "...", "User_Intent": "...", ...}]'
            )
            if intent_text.strip():
                intent_data = parse_intent_data(intent_text)
                if intent_data:
                    st.success(f"âœ… å·²è§£æ {len(intent_data)} ç­†æ„åœ–è³‡æ–™")

    # è™•ç† CSVï¼ˆæ”¯æ´å¤šæª”åˆä½µï¼‰
    if uploaded_csvs:
        try:
            all_dfs = []
            file_stats = []
            
            for uploaded_csv in uploaded_csvs:
                # å˜—è©¦å¤šç¨®ç·¨ç¢¼
                try:
                    single_df = pd.read_csv(uploaded_csv, header=2, encoding='utf-16', sep='\t')
                except:
                    try:
                        single_df = pd.read_csv(uploaded_csv, header=2, encoding='utf-8')
                    except:
                        single_df = pd.read_csv(uploaded_csv, header=2, encoding='latin1')
                
                single_df = clean_google_ads_data(single_df)
                single_df['_source_file'] = uploaded_csv.name  # æ¨™è¨˜ä¾†æº
                all_dfs.append(single_df)
                file_stats.append({
                    'file': uploaded_csv.name,
                    'rows': len(single_df)
                })
            
            # åˆä½µæ‰€æœ‰æª”æ¡ˆ
            df = pd.concat(all_dfs, ignore_index=True)
            
            # å»é‡ï¼ˆåŒé—œéµå­—ä¿ç•™æœå°‹é‡è¼ƒé«˜çš„ï¼‰
            df = df.sort_values('Avg. monthly searches', ascending=False)
            df = df.drop_duplicates(subset=['Keyword'], keep='first')
            
            st.divider()
            
            # é¡¯ç¤ºæª”æ¡ˆçµ±è¨ˆ
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                st.metric("ä¸Šå‚³æª”æ¡ˆæ•¸", len(uploaded_csvs))
            with col_stat2:
                st.metric("åˆä½µå¾Œé—œéµå­—", len(df))
            with col_stat3:
                st.metric("å»é‡å‰ç¸½æ•¸", sum(f['rows'] for f in file_stats))
            
            with st.expander("ğŸ“ æª”æ¡ˆæ˜ç´°"):
                for f in file_stats:
                    st.caption(f"â€¢ {f['file']}: {f['rows']} ç­†")
            
            # é¡¯ç¤ºæ„åœ–æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if intent_data:
                with st.expander("ğŸ¯ æ„åœ–ç ”ç©¶æ‘˜è¦", expanded=True):
                    for item in intent_data[:5]:  # æœ€å¤šé¡¯ç¤º 5 ç­†
                        st.markdown(f"**{item.get('Keyword', 'N/A')}**")
                        st.caption(f"æ„åœ–ï¼š{item.get('User_Intent', 'N/A')}")
                        st.caption(f"æ©Ÿæœƒï¼š{item.get('Opportunity_Gap', 'N/A')}")
            
            # ===== åˆ†æ Tabs =====
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ğŸ“Š è©é »åˆ†æ",
                "ğŸ’° å•†æ¥­åƒ¹å€¼",
                "ğŸ“ˆ è¶¨å‹¢åˆ†ç¾¤",
                "ğŸ”´ğŸ”µ ç´…è—æµ·",
                "ğŸ§  AI æ´å¯Ÿ"
            ])
            
            keywords_col = df['Keyword'] if 'Keyword' in df.columns else df.iloc[:, 0]
            
            # Tab 1: è©é »
            with tab1:
                st.subheader("è©å½™å‡ºç¾é »æ¬¡")
                freq_df = analyze_word_frequency(keywords_col, top_n=25)
                
                col_chart, col_table = st.columns([2, 1])
                with col_chart:
                    st.bar_chart(freq_df.set_index('è©å½™')['é »æ¬¡'])
                with col_table:
                    st.dataframe(freq_df, use_container_width=True, height=400)
            
            # Tab 2: å•†æ¥­åƒ¹å€¼
            with tab2:
                st.subheader("è©å½™å•†æ¥­åƒ¹å€¼æ’è¡Œ")
                st.caption("åƒ¹å€¼åˆ† = é »æ¬¡(30%) Ã— æœå°‹é‡(40%) Ã— å‡ºåƒ¹(30%)")
                
                value_df = calculate_word_value(df, keywords_col)
                
                # ç¯©é¸
                min_count = st.slider("æœ€ä½å‡ºç¾æ¬¡æ•¸", 1, 10, 2)
                filtered_value = value_df[value_df['å‡ºç¾æ¬¡æ•¸'] >= min_count].head(30)
                
                st.dataframe(
                    filtered_value.style.background_gradient(
                        subset=['å•†æ¥­åƒ¹å€¼åˆ†'], 
                        cmap='YlOrRd'
                    ),
                    use_container_width=True,
                    height=500
                )
            
            # Tab 3: è¶¨å‹¢åˆ†ç¾¤
            with tab3:
                st.subheader("è©å½™è¶¨å‹¢åˆ†ç¾¤")
                
                trend_df = analyze_word_trends(df, keywords_col)
                
                col_up, col_down, col_stable = st.columns(3)
                
                rising = trend_df[trend_df['è¶¨å‹¢'] == 'ğŸš€ ä¸Šå‡'].head(15)
                falling = trend_df[trend_df['è¶¨å‹¢'] == 'ğŸ“‰ ä¸‹é™'].head(15)
                stable = trend_df[trend_df['è¶¨å‹¢'] == 'â¡ï¸ ç©©å®š'].head(15)
                
                with col_up:
                    st.markdown("### ğŸš€ ä¸Šå‡è©")
                    st.dataframe(rising[['è©å½™', 'å¹³å‡YoY', 'å¹³å‡æœå°‹é‡']], height=400)
                
                with col_down:
                    st.markdown("### ğŸ“‰ ä¸‹é™è©")
                    st.dataframe(falling[['è©å½™', 'å¹³å‡YoY', 'å¹³å‡æœå°‹é‡']], height=400)
                
                with col_stable:
                    st.markdown("### â¡ï¸ ç©©å®šè©")
                    st.dataframe(stable[['è©å½™', 'å¹³å‡YoY', 'å¹³å‡æœå°‹é‡']], height=400)
            
            # Tab 4: ç´…è—æµ·
            with tab4:
                st.subheader("ç´…è—æµ·é—œéµå­—")
                
                col_red, col_blue = st.columns(2)
                
                with col_red:
                    st.markdown("### ğŸ”¥ ç´…æµ·ï¼ˆé«˜ç«¶çˆ­é«˜å‡ºåƒ¹ï¼‰")
                    red_ocean = df.nlargest(15, 'Top Page Bid (High)')
                    st.dataframe(
                        red_ocean[['Keyword', 'Top Page Bid (High)', 'Avg. monthly searches']],
                        use_container_width=True
                    )
                
                with col_blue:
                    st.markdown("### ğŸ’§ è—æµ·ï¼ˆä½ç«¶çˆ­æœ‰é‡ï¼‰")
                    blue_ocean = df[
                        (df['Avg. monthly searches'] > 100) & 
                        (df['Competition Index'] < 40)
                    ].nlargest(15, 'Avg. monthly searches')
                    st.dataframe(
                        blue_ocean[['Keyword', 'Competition Index', 'Avg. monthly searches']],
                        use_container_width=True
                    )
            
            # Tab 5: AI æ´å¯Ÿ
            with tab5:
                st.subheader("ğŸ§  AI æ·±åº¦æ´å¯Ÿ")
                
                if st.button("å•Ÿå‹• AI åˆ†æ", type="primary"):
                    if not api_key:
                        st.error("è«‹å…ˆè¼¸å…¥ API Key")
                    else:
                        with st.spinner("AI åˆ†æä¸­..."):
                            # æº–å‚™è³‡æ–™æ‘˜è¦
                            freq_top = analyze_word_frequency(keywords_col, 15)
                            value_top = calculate_word_value(df, keywords_col).head(15)
                            trend_summary = analyze_word_trends(df, keywords_col)
                            
                            rising_words = trend_summary[trend_summary['è¶¨å‹¢'] == 'ğŸš€ ä¸Šå‡']['è©å½™'].head(10).tolist()
                            falling_words = trend_summary[trend_summary['è¶¨å‹¢'] == 'ğŸ“‰ ä¸‹é™']['è©å½™'].head(10).tolist()
                            
                            # æ„åœ–è³‡æ–™
                            intent_context = ""
                            if intent_data:
                                intent_context = f"""
                                
                                ã€SERP æ„åœ–ç ”ç©¶çµæœã€‘
                                {json.dumps(intent_data[:5], ensure_ascii=False, indent=2)}
                                """
                            
                            prompt = f"""
                            ä½ æ˜¯å¸‚å ´ç ”ç©¶åˆ†æå¸«ã€‚è«‹æ ¹æ“šä»¥ä¸‹è©å½™çµæ§‹æ•¸æ“šï¼Œæä¾›å•†æ¥­æ´å¯Ÿã€‚
                            
                            ã€é«˜é »è©å½™ã€‘
                            {freq_top.to_string(index=False)}
                            
                            ã€é«˜åƒ¹å€¼è©å½™ã€‘ï¼ˆå•†æ¥­åƒ¹å€¼åˆ† = é »æ¬¡Ã—æœå°‹é‡Ã—å‡ºåƒ¹ï¼‰
                            {value_top[['è©å½™', 'å•†æ¥­åƒ¹å€¼åˆ†', 'å¹³å‡æœå°‹é‡', 'å¹³å‡å‡ºåƒ¹']].to_string(index=False)}
                            
                            ã€ä¸Šå‡è¶¨å‹¢è©ã€‘
                            {rising_words}
                            
                            ã€ä¸‹é™è¶¨å‹¢è©ã€‘
                            {falling_words}
                            {intent_context}
                            
                            è«‹åˆ†æï¼š
                            
                            ## 1. å¸‚å ´çµæ§‹è§£è®€
                            å¾è©å½™é »æ¬¡çœ‹å‡ºä»€éº¼å¸‚å ´ç‰¹å¾µï¼Ÿå“ªäº›æ¦‚å¿µæ˜¯æ ¸å¿ƒï¼Ÿ
                            
                            ## 2. åƒ¹å€¼æ©Ÿæœƒé»
                            é«˜åƒ¹å€¼è©å½™æ­ç¤ºäº†ä»€éº¼å•†æ¥­æ©Ÿæœƒï¼Ÿå»ºè­°å„ªå…ˆæ”»ä½”å“ªäº›è©ï¼Ÿ
                            
                            ## 3. è¶¨å‹¢åˆ¤è®€
                            ä¸Šå‡è©ä»£è¡¨ä»€éº¼æ–°èˆˆéœ€æ±‚ï¼Ÿä¸‹é™è©ä»£è¡¨ä»€éº¼åœ¨é€€å ´ï¼Ÿ
                            
                            ## 4. ç­–ç•¥å»ºè­°
                            åŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œçµ¦å‡º 3 å€‹å…·é«”å¯åŸ·è¡Œçš„å…§å®¹/å»£å‘Šç­–ç•¥ã€‚
                            
                            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œç›´æ¥è¼¸å‡ºåˆ†æï¼Œä¸è¦é‡è¤‡æ•¸æ“šã€‚
                            """
                            
                            result = call_gemini(prompt)
                            st.markdown(result)
            
            # ===== Excel ä¸‹è¼‰ =====
            st.divider()
            if st.button("ğŸ“¥ åŒ¯å‡ºå®Œæ•´åˆ†æ Excel"):
                buffer = io.BytesIO()
                
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    # åŸå§‹è³‡æ–™
                    df.to_excel(writer, sheet_name='Raw_Data', index=False)
                    
                    # è©é »
                    analyze_word_frequency(keywords_col, 50).to_excel(
                        writer, sheet_name='Word_Frequency', index=False
                    )
                    
                    # å•†æ¥­åƒ¹å€¼
                    calculate_word_value(df, keywords_col).head(100).to_excel(
                        writer, sheet_name='Word_Value', index=False
                    )
                    
                    # è¶¨å‹¢
                    analyze_word_trends(df, keywords_col).to_excel(
                        writer, sheet_name='Word_Trends', index=False
                    )
                
                st.download_button(
                    label="â¬‡ï¸ ä¸‹è¼‰ Excel",
                    data=buffer.getvalue(),
                    file_name=f"market_miner_analysis.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                
        except Exception as e:
            st.error(f"CSV è§£æå¤±æ•—: {e}")
            st.info("è«‹ç¢ºèªä¸Šå‚³çš„æ˜¯ Google Keyword Planner åŸå§‹ CSV")


# ==========================================
# 6. æ¨¡å¼ä¸‰ï¼šè©å½™é—œè¯æ¢å‹˜
# ==========================================
elif mode == "ğŸ”— è©å½™é—œè¯æ¢å‹˜":
    st.header("ğŸ”— è©å½™é—œè¯ Ã— å…±ç¾åˆ†æ")
    st.info("åˆ†æè©èˆ‡è©ä¹‹é–“çš„é—œè¯æ€§ï¼Œæ‰¾å‡ºéš±è—çš„èªæ„çµæ§‹")
    
    uploaded_csvs = st.file_uploader(
        "ä¸Šå‚³ Keyword Planner CSVï¼ˆæ”¯æ´å¤šæª”ï¼‰", 
        type=['csv'],
        accept_multiple_files=True
    )
    
    if uploaded_csvs:
        try:
            all_dfs = []
            
            for uploaded_csv in uploaded_csvs:
                try:
                    single_df = pd.read_csv(uploaded_csv, header=2, encoding='utf-16', sep='\t')
                except:
                    try:
                        single_df = pd.read_csv(uploaded_csv, header=2, encoding='utf-8')
                    except:
                        single_df = pd.read_csv(uploaded_csv, header=2, encoding='latin1')
                
                single_df = clean_google_ads_data(single_df)
                all_dfs.append(single_df)
            
            df = pd.concat(all_dfs, ignore_index=True)
            df = df.sort_values('Avg. monthly searches', ascending=False)
            df = df.drop_duplicates(subset=['Keyword'], keep='first')
            
            keywords_col = df['Keyword'] if 'Keyword' in df.columns else df.iloc[:, 0]
            
            st.success(f"âœ… å·²è¼‰å…¥ {len(uploaded_csvs)} å€‹æª”æ¡ˆï¼Œåˆä½µ {len(df)} ç­†é—œéµå­—")
            st.divider()
            
            tab_cooccur, tab_ngram, tab_network = st.tabs([
                "ğŸ”— å…±ç¾åˆ†æ",
                "ğŸ“ N-gram è©çµ„",
                "ğŸ•¸ï¸ é—œè¯è§£è®€"
            ])
            
            # Tab: å…±ç¾
            with tab_cooccur:
                st.subheader("è©å½™å…±ç¾çŸ©é™£")
                st.caption("å“ªäº›è©ç¶“å¸¸ä¸€èµ·å‡ºç¾ï¼Ÿæ­ç¤ºä½¿ç”¨è€…çš„çµ„åˆæœå°‹ç¿’æ…£")
                
                top_n_cooccur = st.slider("é¡¯ç¤ºå‰ N çµ„", 10, 50, 30)
                cooccur_df = analyze_cooccurrence(keywords_col, top_n=top_n_cooccur)
                
                st.dataframe(
                    cooccur_df.style.background_gradient(subset=['å…±ç¾æ¬¡æ•¸'], cmap='Blues'),
                    use_container_width=True,
                    height=500
                )
            
            # Tab: N-gram
            with tab_ngram:
                st.subheader("N-gram è©çµ„åˆ†æ")
                
                col_2gram, col_3gram = st.columns(2)
                
                with col_2gram:
                    st.markdown("### 2-gramï¼ˆé›™è©çµ„ï¼‰")
                    bigram_df = analyze_ngrams(keywords_col, n=2, top_k=20)
                    st.dataframe(bigram_df, use_container_width=True)
                
                with col_3gram:
                    st.markdown("### 3-gramï¼ˆä¸‰è©çµ„ï¼‰")
                    trigram_df = analyze_ngrams(keywords_col, n=3, top_k=20)
                    st.dataframe(trigram_df, use_container_width=True)
            
            # Tab: AI é—œè¯è§£è®€
            with tab_network:
                st.subheader("ğŸ§  AI é—œè¯çµæ§‹è§£è®€")
                
                if st.button("å•Ÿå‹•é—œè¯åˆ†æ", type="primary"):
                    if not api_key:
                        st.error("è«‹å…ˆè¼¸å…¥ API Key")
                    else:
                        with st.spinner("AI è§£è®€è©å½™é—œè¯ä¸­..."):
                            cooccur_data = analyze_cooccurrence(keywords_col, 30)
                            bigram_data = analyze_ngrams(keywords_col, 2, 20)
                            trigram_data = analyze_ngrams(keywords_col, 3, 15)
                            
                            prompt = f"""
                            ä½ æ˜¯èªæ„åˆ†æå°ˆå®¶ã€‚è«‹è§£è®€ä»¥ä¸‹è©å½™é—œè¯æ•¸æ“šã€‚
                            
                            ã€å…±ç¾è©çµ„ã€‘ï¼ˆç¶“å¸¸ä¸€èµ·å‡ºç¾çš„è©ï¼‰
                            {cooccur_data.to_string(index=False)}
                            
                            ã€2-gram è©çµ„ã€‘
                            {bigram_data.to_string(index=False)}
                            
                            ã€3-gram è©çµ„ã€‘
                            {trigram_data.to_string(index=False)}
                            
                            è«‹åˆ†æï¼š
                            
                            ## 1. æ ¸å¿ƒæ¦‚å¿µå¢é›†
                            å¾å…±ç¾é—œä¿‚ä¸­ï¼Œè­˜åˆ¥å‡º 3-5 å€‹æ ¸å¿ƒæ¦‚å¿µç¾¤ï¼ˆå“ªäº›è©å½¢æˆä¸€å€‹ä¸»é¡Œï¼Ÿï¼‰
                            
                            ## 2. ä½¿ç”¨è€…æœå°‹æ¨¡å¼
                            å¾ N-gram è©çµ„çœ‹å‡ºä½¿ç”¨è€…ç”¨ä»€éº¼å¥å¼/çµæ§‹åœ¨æœå°‹ï¼Ÿ
                            
                            ## 3. éš±è—éœ€æ±‚
                            é€™äº›é—œè¯æ­ç¤ºäº†ä»€éº¼æœªè¢«æ˜èªªçš„ä½¿ç”¨è€…éœ€æ±‚ï¼Ÿ
                            
                            ## 4. å…§å®¹ç­–ç•¥å»ºè­°
                            åŸºæ–¼è©å½™é—œè¯ï¼Œå»ºè­°è£½ä½œä»€éº¼é¡å‹çš„å…§å®¹ä¾†è¦†è“‹é€™äº›è©çµ„ï¼Ÿ
                            
                            ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
                            """
                            
                            result = call_gemini(prompt)
                            st.markdown(result)
                            
        except Exception as e:
            st.error(f"CSV è§£æå¤±æ•—: {e}")
